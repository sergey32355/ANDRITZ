{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240ab7d-285f-4ad6-8ae4-e062584a694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from barbar import Bar\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97124e-1709-4cb2-9328-4885ea19d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/mperezcarrasco/PyTorch-DAGMM/blob/master/DAGMM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9fba1-8362-4ffd-a287-7b99fef59bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels=shlp.GetUniqueElements_List(CLASSIF_LABS)\n",
    "if(len(unique_labels)<=1):\n",
    "    print(\"for this algorithms two labels are needed\")\n",
    "feat_norm=[]\n",
    "labs_norm=[]\n",
    "feat_anomaly=[]\n",
    "labs_anomaly=[]\n",
    "label_0=CLASSIF_LABS[0]\n",
    "for k in range(0,len(CLASSIF_LABS)):\n",
    "    if(CLASSIF_LABS[k]==label_0):\n",
    "        feat_norm.append(np.asarray(CLASSIF_FEAT[k][:]))\n",
    "        labs_norm.append(0)\n",
    "    else:\n",
    "        feat_anomaly.append(np.asarray(CLASSIF_FEAT[k][:]))\n",
    "        labs_anomaly.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce886f-206c-4740-b1f0-53c265f6b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1 and classname != 'Conv':\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        torch.nn.init.normal_(m.bias.data, 0.0, 0.02)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        torch.nn.init.normal_(m.bias.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.01)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb5120-4682-4ea2-a3ca-d3534939adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGMM(nn.Module):\n",
    "    def __init__(self, input_size=1000, n_gmm=2, z_dim=30):\n",
    "        \"\"\"Network for DAGMM (KDDCup99)\"\"\"\n",
    "        super(DAGMM, self).__init__()\n",
    "        #Encoder network\n",
    "        print(\"input_size=\"+str(input_size))\n",
    "        self.fc0 = nn.Linear(input_size, 100)\n",
    "        self.fc1 = nn.Linear(100, 80)\n",
    "        self.fc2 = nn.Linear(80, 70)\n",
    "        self.fc3 = nn.Linear(70, 50)\n",
    "        self.fc4 = nn.Linear(50, z_dim)\n",
    "\n",
    "        #Decoder network\n",
    "        self.fc5 = nn.Linear(z_dim, 50)\n",
    "        self.fc6 = nn.Linear(50, 70)\n",
    "        self.fc7 = nn.Linear(70, 80)\n",
    "        self.fc8 = nn.Linear(80, 100)\n",
    "        self.fc9 = nn.Linear(100, input_size)\n",
    "        self.fc9_1 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        #Estimation network\n",
    "        self.fc10 = nn.Linear(z_dim+2, 10)\n",
    "        self.fc11 = nn.Linear(10, n_gmm)\n",
    "\n",
    "    def encode(self, x):        \n",
    "        h = torch.tanh(self.fc0(x))\n",
    "        h = torch.tanh(self.fc1(h))\n",
    "        h = torch.tanh(self.fc2(h))\n",
    "        h = torch.tanh(self.fc3(h))\n",
    "        return self.fc4(h)\n",
    "\n",
    "    def decode(self, x):\n",
    "        h = torch.tanh(self.fc5(x))\n",
    "        h = torch.tanh(self.fc6(h))\n",
    "        h = torch.tanh(self.fc7(h))\n",
    "        h = torch.tanh(self.fc8(h))\n",
    "        h = torch.tanh(self.fc9(h))\n",
    "        return self.fc9_1(h)\n",
    "    \n",
    "    def estimate(self, z):\n",
    "        h = F.dropout(torch.tanh(self.fc10(z)), 0.5)\n",
    "        return F.softmax(self.fc11(h), dim=1)\n",
    "    \n",
    "    def compute_reconstruction(self, x, x_hat):\n",
    "        relative_euclidean_distance = (x-x_hat).norm(2, dim=1) / x.norm(2, dim=1)\n",
    "        cosine_similarity = F.cosine_similarity(x, x_hat, dim=1)\n",
    "        return relative_euclidean_distance, cosine_similarity\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_c = self.encode(x)\n",
    "        x_hat = self.decode(z_c)\n",
    "        rec_1, rec_2 = self.compute_reconstruction(x, x_hat)\n",
    "        z = torch.cat([z_c, rec_1.unsqueeze(-1), rec_2.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimate(z)\n",
    "        return z_c, x_hat, z, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7d6b9-37f0-47ac-a809-a3d06a17d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerDAGMM:\n",
    "    \"\"\"Trainer class for DAGMM.\"\"\"\n",
    "    def __init__(self, args, data, device):\n",
    "        self.args = args\n",
    "        self.train_loader = data\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Training the DAGMM model\"\"\"\n",
    "        self.model = DAGMM(input_size=self.args.input_size, \n",
    "                           n_gmm=self.args.n_gmm, \n",
    "                           z_dim=self.args.latent_dim\n",
    "                          ).to(self.device)#self.args.n_gmm, self.args.latent_dim).to(self.device)\n",
    "        \n",
    "        self.model.apply(weights_init_normal)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "\n",
    "        self.compute = ComputeLoss(self.model, self.args.lambda_energy, self.args.lambda_cov, \n",
    "                                   self.device, self.args.n_gmm)\n",
    "        self.model.train()\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            total_loss = 0\n",
    "            for x,y in Bar(self.train_loader):                    \n",
    "                x = x.float().to(self.device)\n",
    "                optimizer.zero_grad()                \n",
    "                _, x_hat, z, gamma = self.model(x)\n",
    "                loss = self.compute.forward(x, x_hat, z, gamma)\n",
    "                loss.backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            print('Training DAGMM... Epoch: {}, Loss: {:.3f}'.format(\n",
    "                   epoch, total_loss/len(self.train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188f10b-4385-4374-91ab-6d9b2092e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-6\n",
    "    lr_milestones=[50]\n",
    "    batch_size=10\n",
    "    latent_dim=1\n",
    "    n_gmm=5\n",
    "    lambda_energy=0.01\n",
    "    lambda_cov=0.0005\n",
    "    input_size=np.shape(np.asarray(feat_norm))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942e36e-6474-45fb-a965-f37d178c6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(False)\n",
    "args=Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#datal_train,datal_test = get_KDDCup99(args,feat_norm,labs_norm,feat_anomaly,labs_anomaly)\n",
    "norm_x = torch.Tensor(feat_norm).to(device) \n",
    "norm_y = torch.Tensor(labs_norm).to(device) \n",
    "norm_dataset = torch.utils.data.TensorDataset(norm_x,norm_y)\n",
    "norm_dataloader = torch.utils.data.DataLoader(norm_dataset)\n",
    "dagmm = TrainerDAGMM(args, norm_dataloader, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fe3a4-86d2-4c1d-a4c5-1f36f4380dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi = 0     #gamma_sum / N_samples\n",
    "train_mu =  0     #mu_sum / gamma_sum.unsqueeze(-1)\n",
    "train_cov = 0     #cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    N_samples = 0\n",
    "    gamma_sum = 0\n",
    "    mu_sum = 0\n",
    "    cov_sum = 0\n",
    "    \n",
    "    n_gmm=args.n_gmm\n",
    "    model= dagmm.model\n",
    "    compute = ComputeLoss(model, None, None, device, n_gmm)\n",
    "    \n",
    "    for x, _ in norm_dataloader:\n",
    "        _, _, z, gamma = model(x)\n",
    "        phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
    "        \n",
    "        batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "        gamma_sum += batch_gamma_sum\n",
    "        mu_sum += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
    "        cov_sum += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "        N_samples += x.size(0)\n",
    "    \n",
    "    train_phi = gamma_sum / N_samples\n",
    "    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4829162-d647-42b8-8397-36abfe849f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_x = torch.Tensor(feat_anomaly).to(device) \n",
    "anomaly_y = torch.Tensor(labs_anomaly).to(device) \n",
    "anomaly_dataset = torch.utils.data.TensorDataset(anomaly_x,anomaly_y)\n",
    "anomaly_dataloader = torch.utils.data.DataLoader(anomaly_dataset)\n",
    "print(\"Shape of test: \"+str(anomaly_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8e9e4-f56e-44cb-8489-2d82ace841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run anomaly detection\n",
    "def model_run(anomaly_dataloader,train_phi,train_mu,train_cov):\n",
    "    energy_test = []\n",
    "    for x, y in anomaly_dataloader:\n",
    "            x = x.float().to(device)\n",
    "    \n",
    "            _, _, z, gamma = model(x)\n",
    "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, train_phi,\n",
    "                                                              train_mu, train_cov,\n",
    "                                                              sample_mean=False)\n",
    "            if(str(device) == 'cuda'):            \n",
    "                energy_test.append(sample_energy.detach().cpu())\n",
    "            else:\n",
    "                 energy_test.append(sample_energy)\n",
    "    shp=len(energy_test)\n",
    "    energy=[]\n",
    "    for k in range(0,shp):\n",
    "        #for l in range(0,energy_test[l]):\n",
    "        energy.append(energy_test[k].numpy())\n",
    "    energy=np.asarray(energy)\n",
    "    print(cov_diag.shape)\n",
    "    return energy\n",
    "\n",
    "labels_test=model_run(anomaly_dataloader,train_phi,train_mu,train_cov)\n",
    "labels_train=model_run(norm_dataloader,train_phi,train_mu,train_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02a2ea-7a51-4be1-a10c-e0a3c65f9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_test)\n",
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b2db5-dd3f-4a88-ac3d-b9cd6bcb5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the parameters gamma, mu and cov using the trainin (clean) data\n",
    "with torch.no_grad():\n",
    "    \n",
    "    N_samples = 0\n",
    "    gamma_sum = 0\n",
    "    mu_sum = 0\n",
    "    cov_sum = 0\n",
    "    \n",
    "    n_gmm=args.n_gmm\n",
    "    model= dagmm.model\n",
    "    compute = ComputeLoss(model, None, None, device, n_gmm)\n",
    "    \n",
    "    for x, _ in norm_dataloader:\n",
    "        _, _, z, gamma = model(x)\n",
    "        phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
    "        \n",
    "        batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "        gamma_sum += batch_gamma_sum\n",
    "        mu_sum += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
    "        cov_sum += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "        N_samples += x.size(0)\n",
    "    \n",
    "    train_phi = gamma_sum / N_samples\n",
    "    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    # Obtaining Labels and energy scores for train data\n",
    "    energy_train = []\n",
    "    labels_train = []\n",
    "\n",
    "    for x, y in norm_dataloader:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, phi=train_phi,\n",
    "                                                              mu=train_mu, cov=train_cov, \n",
    "                                                              sample_mean=False)\n",
    "            \n",
    "            energy_train.append(sample_energy.detach().cpu())\n",
    "            labels_train.append(y)\n",
    "\n",
    "    \n",
    "    if(str(device) == 'cuda'):        \n",
    "        tmp_labs_train = []\n",
    "        for l in range(0,len(labels_train)):\n",
    "            tmp_labs_train.append(labels_train[l].detach().cpu().numpy())\n",
    "        labels_train=torch.tensor(tmp_labs_train)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    energy_train = torch.cat(energy_train).numpy()\n",
    "    labels_train = torch.cat(labels_train).numpy()\n",
    "\n",
    "    # Obtaining Labels and energy scores for test data\n",
    "    energy_test = []\n",
    "    labels_test = []\n",
    "    \n",
    "    for x, y in anomaly_dataloader:\n",
    "        x = x.float().to(device)\n",
    "\n",
    "        _, _, z, gamma = model(x)\n",
    "        sample_energy, cov_diag  = compute.compute_energy(z, gamma, train_phi,\n",
    "                                                          train_mu, train_cov,\n",
    "                                                          sample_mean=False)\n",
    "            \n",
    "        energy_test.append(sample_energy.detach().cpu())\n",
    "        labels_test.append(y)\n",
    "        \n",
    "    energy_test = torch.cat(energy_test).numpy()\n",
    "    labels_test = torch.cat(labels_test).numpy()\n",
    "    \n",
    "    scores_total = np.concatenate((energy_train, energy_test), axis=0)\n",
    "    labels_total = np.concatenate((labels_train, labels_test), axis=0)\n",
    "\n",
    "threshold = np.percentile(scores_total, 100 - 20)\n",
    "pred = (energy_test > threshold).astype(int)\n",
    "gt = labels_test.astype(int)\n",
    "precision, recall, f_score, _ = prf(gt, pred, average='binary')\n",
    "print(\"Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(precision, recall, f_score))\n",
    "print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels_total, scores_total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d08791-7d36-43e9-92a3-9eb1659617ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(feat_anomaly).to(device) \n",
    "tensor_y = torch.Tensor(labs_anomaly).to(device) \n",
    "my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "my_dataloader = torch.utils.data.DataLoader(my_dataset)\n",
    "_, _, z, gamma=dagmm.model(tensor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a3d0f-c3dd-40cc-a6c9-ef92a6ada8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363656cf-ea33-4ed7-bdac-73ba7ebd5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/xuhongzuo/DeepOD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
