{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e8530-9d57-4618-a1b6-e687e741279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os import walk\n",
    "import sys\n",
    "from time import sleep\n",
    "import traceback\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import uuid \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn import preprocessing\n",
    "import distinctipy as dc\n",
    "import time\n",
    "import logging\n",
    "import threading\n",
    "import sys\n",
    "import trace\n",
    "import glob\n",
    "\n",
    "from PySide6.QtWidgets import QApplication, QMainWindow, QTextEdit,QFileDialog\n",
    "from PySide6.QtCore import QThread, Signal, QObject\n",
    "from gui_files.empa_gui import Ui_EmpaGUI\n",
    "from mod.mod_gui_data_loader_bpp_lines import DataLoaderBPPLines\n",
    "from mod.mod_gui_feature_extractor_bpp_lines import FeatureExtractorBPPLines\n",
    "from mod.mod_gui_model_selector_bpp_lines import ModelSelectorBPPLines, PredictorScorerBPPLines\n",
    "from PySide6.QtWidgets import QMessageBox\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg\n",
    "from matplotlib.figure import Figure\n",
    "from PySide6.QtWidgets import QDialog, QVBoxLayout\n",
    "from PySide6.QtWidgets import QMessageBox\n",
    "\n",
    "#classifiers\n",
    "from xgboost import XGBClassifier\n",
    "import torchaudio\n",
    "import pywt\n",
    "import ptwt\n",
    "\n",
    "import SHelpers as shlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc9ba1-39b4-4ae7-a5e3-c933b34b56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DataPreproc:\n",
    "    def __init__ (self):\n",
    "        #SplitEntireSignalIntoSnippets\n",
    "        self.sign_1=torch.empty\n",
    "        self.sign_2=torch.empty\n",
    "        self.sign_3=torch.empty\n",
    "        self.sign_4=torch.empty      \n",
    "        self.preproc_1=torch.empty\n",
    "        self.proceed=False\n",
    "        #SplitLabPlateSegmentIntoSnips\n",
    "        self.sign_5=torch.empty   \n",
    "        self.sign_6=torch.empty\n",
    "        self.np_signal1=np.empty\n",
    "        self.np_signal2=np.empty\n",
    "        #preprocessing\n",
    "        self.preproc=torch.empty                \n",
    "        self.preproc_in=torch.empty     \n",
    "        self.preproc_flat=torch.empty\n",
    "        self.preproc_final=torch.empty\n",
    "        #SplitLabPlateAllSegmentIntoSnips\n",
    "        self.np_signal3=np.empty        \n",
    "        self.sign_7=torch.empty         \n",
    "        self.segm_snips_list=[]\n",
    "        #SplitAllLabPlateSegmentIntoSnips        \n",
    "        self.sign_8=torch.empty \n",
    "        self.sign_9=torch.empty \n",
    "        self.segm_labels_list=[]\n",
    "        self.segm_sign_list=[]\n",
    "        #SplitAllLabPlateOfAllSegmentsIntoSnips\n",
    "        self.np_sign_4=torch.empty \n",
    "        self.segm_labels_list1=[]\n",
    "        self.segm_sign_list1=[]       \n",
    "        self.segm_labels_list2=[]\n",
    "        self.segm_sign_list2=[]       \n",
    "        \n",
    "    #this splits the entire number of segments pf the plate\n",
    "    def SplitAllPlateSegmentsIntoSnippets(self,plate, \n",
    "                                          channs_indx=[], \n",
    "                                          snip_size=5,\n",
    "                                          torch_tensor=True, \n",
    "                                          preproc_type=\"None\",         \n",
    "                                          proc_time=False,\n",
    "                                         ):\n",
    "                \n",
    "        segm_num=len(plate.sigments_sign)\n",
    "        \n",
    "        for t in range(0,segm_num):\n",
    "            self.np_signal3=np.empty\n",
    "            self.np_signal3=plate.sigments_sign[t]\n",
    "            self.sign_7=self.SplitEntireSignalIntoSnippets(signal=self.np_signal3, \n",
    "                                                      channs_indx=channs_indx, \n",
    "                                                      snip_size=snip_size,\n",
    "                                                      torch_tensor=True, \n",
    "                                                      preproc_type=preproc_type,\n",
    "                                                      proc_time=False)            \n",
    "            if(torch_tensor==False):\n",
    "                feat = self.sign_7.cpu().numpy().copy()\n",
    "                self.segm_snips_list.append(feat)\n",
    "            else:\n",
    "                self.segm_snips_list.append(self.sign_7.clone())\n",
    "        return self.segm_snips_list.copy()\n",
    "\n",
    "#pass through all plates and \n",
    "    def SplitAllLabPlateOfAllSegmentsIntoSnips(self,plates,\n",
    "                                               snip_size=5,\n",
    "                                               channs_indx=0,\n",
    "                                               torch_tensor=False, \n",
    "                                               preproc_type=\"None\",                                                \n",
    "                                              ):\n",
    "        self.segm_labels_list2=[]\n",
    "        self.segm_sign_list2=[]       \n",
    "        \n",
    "        for pl in plates:\n",
    "            self.segm_labels_list1=[]\n",
    "            self.segm_sign_list1=[]       \n",
    "            self.segm_sign_list1,self.segm_labels_list1=self.SplitAllLabPlateSegmentsIntoSnips(pl,\n",
    "                                                                                               snip_size=snip_size,\n",
    "                                                                                               channs_indx=channs_indx,\n",
    "                                                                                               torch_tensor=torch_tensor, \n",
    "                                                                                               preproc_type=preproc_type,  \n",
    "                                                                                               )\n",
    "            self.segm_sign_list1,self.segm_labels_list1=self.Helper_FlatListOfLabeledFeat(self.segm_sign_list1,\n",
    "                                                                                          self.segm_labels_list1\n",
    "                                                                                         )\n",
    "            for k in range(0,len(self.segm_sign_list1)):\n",
    "                self.segm_labels_list2.append(self.segm_labels_list1[k])\n",
    "                self.segm_sign_list2.append(self.segm_sign_list1[k])      \n",
    "\n",
    "        return self.segm_sign_list2.copy(),self.segm_labels_list2.copy()\n",
    "\n",
    "    #this splits all labeled segments into snippets for the given plate\n",
    "    def SplitAllLabPlateSegmentsIntoSnips(self,plate,\n",
    "                                         snip_size=5,\n",
    "                                         channs_indx=-1,\n",
    "                                         torch_tensor=False, \n",
    "                                         preproc_type=\"None\",\n",
    "                                        ):\n",
    "\n",
    "        self.segm_labels_list=[]\n",
    "        self.segm_sign_list=[]\n",
    "        segm_num=len(plate.sigments_sign)\n",
    "        \n",
    "        for t in range(0,segm_num):           \n",
    "            self.sign_8=torch.empty \n",
    "            self.sign_9=torch.empty            \n",
    "            self.sign_8,self.sign_9=self.SplitLabPlateSegmentIntoSnips(plate,\n",
    "                                                                       snip_size=snip_size,\n",
    "                                                                       segm_index=t,\n",
    "                                                                       channs_indx=channs_indx,\n",
    "                                                                       torch_tensor=True, \n",
    "                                                                       preproc_type=preproc_type,      \n",
    "                                                                      )\n",
    "            \n",
    "            if(self.sign_9 is None or self.sign_8 is None):\n",
    "                if(torch_tensor==True):\n",
    "                    self.segm_sign_list.append(torch.empty)\n",
    "                    self.segm_labels_list.append(torch.empty)\n",
    "                else:\n",
    "                    self.segm_sign_list.append(np.empty)\n",
    "                    self.segm_labels_list.append(np.empty)\n",
    "            else:\n",
    "                if(torch_tensor==True):\n",
    "                    self.segm_sign_list.append(self.sign_8.clone())\n",
    "                    self.segm_labels_list.append(self.sign_9.clone())\n",
    "                    \n",
    "                else:                    \n",
    "                    self.segm_sign_list.append(self.sign_8.cpu().numpy().copy())\n",
    "                    self.segm_labels_list.append(self.sign_9.cpu().numpy().copy())\n",
    "        return self.segm_sign_list.copy(),self.segm_labels_list.copy()\n",
    "            \n",
    "    #split obnly labeled data within a segment into snyppets\n",
    "    def SplitLabPlateSegmentIntoSnips(self,plate,\n",
    "                                           snip_size=5,\n",
    "                                           segm_index=0,\n",
    "                                           channs_indx=-1,\n",
    "                                           torch_tensor=False, \n",
    "                                           preproc_type=\"None\",\n",
    "                                     ):\n",
    "\n",
    "        self.np_signal1=np.empty\n",
    "        self.np_signal2=np.empty\n",
    "        self.sign_6=torch.empty\n",
    "        \n",
    "        lab=[]        \n",
    "        labeled_data_l=len(plate.sigments_labels[segm_index])    \n",
    "        self.np_signal1 = np.asarray(plate.sigments_sign[segm_index])        \n",
    "        for hj in range(0,labeled_data_l):             \n",
    "            #read labeled data first\n",
    "            label=plate.sigments_labels[segm_index][hj][2]\n",
    "            first_el=plate.sigments_labels[segm_index][hj][0]\n",
    "            last_el=plate.sigments_labels[segm_index][hj][1]\n",
    "            self.np_signal2=self.np_signal1[:,first_el:last_el].copy()         \n",
    "            self.sign_5=torch.empty   \n",
    "            #print(preproc_type)\n",
    "            #print(self.np_signal2.shape)\n",
    "            self.sign_5= self.SplitEntireSignalIntoSnippets(  signal=self.np_signal2, \n",
    "                                                              channs_indx=channs_indx, \n",
    "                                                              snip_size=snip_size,\n",
    "                                                              torch_tensor=True, \n",
    "                                                              preproc_type=preproc_type,\n",
    "                                                              proc_time=False\n",
    "                                                            )       \n",
    "            \n",
    "            if(self.sign_5 is not None):\n",
    "                if(self.sign_6==torch.empty):  self.sign_6=self.sign_5.clone()\n",
    "                else: self.sign_6 = torch.cat([self.sign_6,self.sign_5],dim=0)     \n",
    "                for k in range(0,self.sign_5.shape[0]):\n",
    "                    lab.append(label)\n",
    "            else: pass                \n",
    "                    \n",
    "        if(torch_tensor==True):\n",
    "                    if((self.sign_6!=torch.empty) and (self.sign_6 is not None)):\n",
    "                        torch_labels_1=torch.tensor(np.asarray(lab))\n",
    "                        if(torch.cuda.is_available()): torch_labels_1.cuda()                    \n",
    "                        feat=self.sign_6.clone()\n",
    "                        return feat, torch_labels_1\n",
    "                    else: return None,None\n",
    "        else:    \n",
    "            if(self.sign_6==torch.empty):\n",
    "                return None,None\n",
    "            else:\n",
    "                feat=self.sign_6.cpu().numpy().copy()\n",
    "                return feat, np.asarray(lab.copy())               \n",
    "        \n",
    "    #split into snippets the given signal\n",
    "    def SplitEntireSignalIntoSnippets(self,signal=None, \n",
    "                                      channs_indx=[], \n",
    "                                      snip_size=5,\n",
    "                                      torch_tensor=True, \n",
    "                                      preproc_type=\"None\",\n",
    "                                      proc_time=False\n",
    "                                     ):\n",
    "        self.sign_1=torch.empty\n",
    "        self.sign_2=torch.empty\n",
    "        self.sign_3=torch.empty\n",
    "        self.sign_4=torch.empty        \n",
    "\n",
    "        if(proc_time==True):\n",
    "            start_t = time.time()\n",
    "        self.sign_1=torch.tensor(np.asarray(signal))\n",
    "        if(torch.cuda.is_available()): self.sign_1.cuda()\n",
    "        sign_shape=self.sign_1.shape         \n",
    "        num_snipps=np.round(sign_shape[1]/snip_size)\n",
    "        for k in range(0,sign_shape[0]):            \n",
    "            self.proceed = False\n",
    "            if(type(channs_indx) is not list):\n",
    "                if(channs_indx == -1): self.proceed=True  \n",
    "                else: \n",
    "                    if(k==channs_indx): self.proceed=True  \n",
    "            else:\n",
    "                if(k in channs_indx): self.proceed=True  \n",
    "                \n",
    "            if(self.proceed==True):                \n",
    "                self.sign_2=torch.split(self.sign_1[k],snip_size,dim=0)        \n",
    "                self.sign_2 = list(self.sign_2)\n",
    "                while(True): \n",
    "                    if(len(self.sign_2)<num_snipps):break                                                      \n",
    "                    del self.sign_2[-1]\n",
    "                self.sign_3=torch.stack(self.sign_2, dim=0) \n",
    "                #preprocessing\n",
    "                if(preproc_type==\"None\" or  preproc_type==\"\" or preproc_type==\" \"): pass\n",
    "                else: self.sign_3=self.DataPreprocessing(self.sign_3,preproc_type=preproc_type)\n",
    "                #assign further\n",
    "                if(self.sign_4 == torch.empty): self.sign_4 = self.sign_3 \n",
    "                else: self.sign_4 = torch.cat([self.sign_4,self.sign_3],dim=1)#stack((self.sign_4,self.sign_3),dim=1) \n",
    "        if(proc_time==True):\n",
    "            end_t = time.time()\n",
    "            print(\"Feat. extr. time(s): \"+str(end_t - start_t))\n",
    "        if(torch_tensor==True):\n",
    "            return self.sign_4.clone()\n",
    "        else:\n",
    "            return self.sign_4.cpu().numpy().copy()      \n",
    "\n",
    "    #------------------obtain spectrograms-------------------------\n",
    "    #------------------as a preprocessing step---------------------\n",
    "    #--------------------------------------------------------------\n",
    "    def DataPreprocessing(self,signal,preproc_type=\"torch_MEL\"):\n",
    "        if(torch.cuda.is_available()):\n",
    "            if(signal.is_cuda==False):\n",
    "                signal.cuda()\n",
    "                \n",
    "        self.preproc=torch.empty\n",
    "        self.preproc_final=torch.empty\n",
    "\n",
    "        shp=signal.shape\n",
    "        \n",
    "        if(preproc_type==\"torch_MEL\"):\n",
    "            #parameters\n",
    "            n_fft = 128         #256       #Toni_MEL_nfft\n",
    "            win_length = None\n",
    "            hop_length = 2024   #2024  #Toni_MEL_Samp_hop_length\n",
    "            n_mels = 512 #512   #self.Toni_MEL_Samp_n_mels\n",
    "            sample_rate = 1000000 #40000 #self.Toni_MEL_samp_rate\n",
    "            #MEL decomposition            \n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))                \n",
    "                #https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "                MEL_transoform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                                                n_fft=n_fft,\n",
    "                                                                                win_length=win_length,\n",
    "                                                                                hop_length=hop_length,\n",
    "                                                                                center=True,\n",
    "                                                                                pad_mode=\"reflect\",\n",
    "                                                                                power=2.0,\n",
    "                                                                                norm=\"slaney\",\n",
    "                                                                                n_mels=n_mels,\n",
    "                                                                                mel_scale=\"htk\",).to(torch.double)   \n",
    "                \n",
    "                self.preproc = MEL_transoform(self.preproc_in)                \n",
    "                self.preproc_flat = self.preproc.reshape(-1)    \n",
    "                self.preproc_flat=self.preproc_flat.unsqueeze(0)                \n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc_flat\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc_flat],dim=0)                    \n",
    "            return self.preproc_final.clone()\n",
    "        #fft\n",
    "        if(preproc_type==\"torch_FFT\"):\n",
    "            #https://www.kaggle.com/code/yassinealouini/signal-processing-theory-and-practice-with-pytorch\n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))                \n",
    "                self.preproc=torch.fft.fft(self.preproc_in)\n",
    "                shp_proc=self.preproc.shape\n",
    "                cut_half=int(shp_proc[1]/2)\n",
    "                self.preproc=self.preproc[:,0:]\n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc],dim=0)                       \n",
    "            return self.preproc_final.clone()\n",
    "        #mfcc spectrogram\n",
    "        if(preproc_type==\"torch_MFCC\"):\n",
    "            #parameters\n",
    "            n_mfcc=13\n",
    "            sample_rate = 1000000 #40000 \n",
    "            \n",
    "            #MEL decomposition            \n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))                \n",
    "                #https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "                MFCC_transoform = torchaudio.transforms.MFCC(sample_rate=sample_rate,\n",
    "                                                            n_mfcc=n_mfcc,\n",
    "                                                            melkwargs={\"n_fft\": 50, \"hop_length\": 50, \"n_mels\": 23, \"center\": False},\n",
    "                                                           ).to(torch.double)   \n",
    "                \n",
    "                self.preproc = MFCC_transoform(self.preproc_in)                \n",
    "                self.preproc_flat = self.preproc.reshape(-1)    \n",
    "                self.preproc_flat=self.preproc_flat.unsqueeze(0)                \n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc_flat\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc_flat],dim=0)                    \n",
    "            return self.preproc_final .clone()\n",
    "\n",
    "        #widow fft spectrogram\n",
    "        if(preproc_type==\"torch_WFFT\"):\n",
    "            #parameters\n",
    "            n_fft=50\n",
    "            win_len=None\n",
    "            hop_len=None\n",
    "            power=2.0\n",
    "            \n",
    "            #MEL decomposition            \n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))                \n",
    "                #https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "                WFFT_transoform = torchaudio.transforms.Spectrogram(n_fft=n_fft,\n",
    "                                                                    win_length=win_len,\n",
    "                                                                    hop_length=hop_len,\n",
    "                                                                    center=True,\n",
    "                                                                    pad_mode=\"reflect\",\n",
    "                                                                    power=power,\n",
    "                                                                    ).to(torch.double)              \n",
    "                self.preproc = WFFT_transoform(self.preproc_in)                \n",
    "                self.preproc_flat = self.preproc.reshape(-1)    \n",
    "                self.preproc_flat=self.preproc_flat.unsqueeze(0)                \n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc_flat\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc_flat],dim=0)                    \n",
    "            return self.preproc_final .clone()\n",
    "            \n",
    "        #Griffin-Lim transform\n",
    "        if(preproc_type==\"torch_GrifLim\"):\n",
    "            #parameters\n",
    "            n_fft=64 #should be order of two\n",
    "                        \n",
    "            #MEL decomposition            \n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))       \n",
    "                #https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "                GL_transoform = torchaudio.transforms.GriffinLim(n_fft=n_fft).to(torch.double)              \n",
    "                self.preproc = GL_transoform(self.preproc_in)                \n",
    "                self.preproc_flat = self.preproc.reshape(-1)    \n",
    "                self.preproc_flat=self.preproc_flat.unsqueeze(0)                \n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc_flat\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc_flat],dim=0)                    \n",
    "            return self.preproc_final.clone()\n",
    "\n",
    "        #Griffin-Lim transform\n",
    "        if(preproc_type==\"torch_LFCC\"):\n",
    "            #parameters            \n",
    "            sample_rate = 1000000\n",
    "            n_lfcc=13\n",
    "                        \n",
    "            #MEL decomposition            \n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))       \n",
    "                #https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "                LFCC_transoform = torchaudio.transforms.LFCC(sample_rate=sample_rate,\n",
    "                                                             n_lfcc=n_lfcc,\n",
    "                                                             speckwargs={\"n_fft\": 40, \"hop_length\": 30, \"center\": False},\n",
    "                                                            ).to(torch.double)              \n",
    "                self.preproc = LFCC_transoform(self.preproc_in)                \n",
    "                self.preproc_flat = self.preproc.reshape(-1)    \n",
    "                self.preproc_flat=self.preproc_flat.unsqueeze(0)                \n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc_flat\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc_flat],dim=0)                    \n",
    "            return self.preproc_final.clone()\n",
    "\n",
    "        #Spec centroid\n",
    "        if(preproc_type==\"torch_SpecCentr\"):\n",
    "            #parameters            \n",
    "            sample_rate = 1000000\n",
    "            n_fft = 50\n",
    "                        \n",
    "            #MEL decomposition            \n",
    "            for l in range(0,shp[0]):\n",
    "                self.preproc_in=torch.empty\n",
    "                self.preproc_in=signal[l,:].reshape((1, -1))       \n",
    "                #https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
    "                SC_transoform = torchaudio.transforms.SpectralCentroid(sample_rate=sample_rate,\n",
    "                                                                       n_fft=n_fft,\n",
    "                                                                      ).to(torch.double)              \n",
    "                self.preproc = SC_transoform(self.preproc_in)                \n",
    "                self.preproc_flat = self.preproc.reshape(-1)    \n",
    "                self.preproc_flat=self.preproc_flat.unsqueeze(0)                \n",
    "                if(self.preproc_final==torch.empty): self.preproc_final = self.preproc_flat\n",
    "                else:   self.preproc_final=torch.cat([self.preproc_final,self.preproc_flat],dim=0)                    \n",
    "            return self.preproc_final.clone()\n",
    "\n",
    "    #helper functions\n",
    "    def Helper_FlatListOfLabeledFeat(self,feat,labels):\n",
    "        feat_new=[]\n",
    "        labs=[]\n",
    "        h_l=len(feat)\n",
    "        for i in range(0,h_l):      \n",
    "            if(feat[i] is not None): \n",
    "                shp = np.shape(feat[i])       \n",
    "                if(len(shp)>=1):\n",
    "                    for j in range(0,shp[0]):\n",
    "                        feat_new.append(feat[i][j][:])\n",
    "                        labs.append(labels[i][j])\n",
    "        return feat_new,labs\n",
    "            \n",
    "#dp=DataPreproc()\n",
    "#fd= dp.SplitEntireSignalIntoSnippets(signal=PLATES_ARRAY[0].sigments_sign[0],channs_indx=0,torch_tensor=False,snip_size=1000)\n",
    "#fd,labs= dp.SplitLabPlateSegmentIntoSnips(PLATES_ARRAY[0],snip_size=5,segm_index=0,channs_indx=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51392a6a-b5ec-4d89-bc32-0e8c16120ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=DataPreproc()\n",
    "#fd= dp.SplitAllPlateSegmentsIntoSnippets(PLATES_ARRAY[0],channs_indx=[1],torch_tensor=False,snip_size=100)\n",
    "#feat,labels= dp.SplitAllLabPlateSegmentsIntoSnips(PLATES_ARRAY[0],channs_indx=[1],torch_tensor=False,snip_size=100)\n",
    "#f,l=dp.Helper_FlatListOfLabeledFeat(feat,labels)\n",
    "#print(np.shape(np.asarray(f)))\n",
    "feat,labels= dp.SplitAllLabPlateOfAllSegmentsIntoSnips(PLATES_ARRAY,channs_indx=[1],torch_tensor=False,snip_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10ad99-3370-4845-80f2-f372de5e4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=DataPreproc()\n",
    "fd= dp.SplitEntireSignalIntoSnippets(signal=PLATES_ARRAY[0].sigments_sign[0],\n",
    "                                     channs_indx=[0,1],\n",
    "                                     torch_tensor=False,\n",
    "                                     snip_size=10,\n",
    "                                     preproc_type=\"tocrh_DWT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
