{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc94558-d4bf-4342-8219-407ef8bae7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import datetime\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7f5841-a3ce-4a47-852a-840e99410ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/danieltan07/dagmm\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "def mkdir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651c2b9e-a290-4380-921a-d6ee08f54554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cholesky(torch.autograd.Function):\n",
    "    def forward(ctx, a):\n",
    "        l = torch.potrf(a, False)\n",
    "        ctx.save_for_backward(l)\n",
    "        return l\n",
    "    def backward(ctx, grad_output):\n",
    "        l, = ctx.saved_variables\n",
    "        linv = l.inverse()\n",
    "        inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n",
    "            1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n",
    "        s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d3052f9-7260-40fa-bcd6-cf151ec4ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaGMM(nn.Module):\n",
    "    \"\"\"Residual Block.\"\"\"\n",
    "    def __init__(self, input_layer=100, n_gmm = 2, latent_dim=30):\n",
    "        super(DaGMM, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Linear(input_layer,600)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Linear(600,300)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Linear(300,100)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Linear(100,5)]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Linear(5,100)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Linear(100,300)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Linear(300,600)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Linear(600,input_layer)]\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Linear(latent_dim,10)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Dropout(p=0.5)]        \n",
    "        layers += [nn.Linear(10,n_gmm)]\n",
    "        layers += [nn.Softmax(dim=1)]\n",
    "\n",
    "\n",
    "        self.estimation = nn.Sequential(*layers)\n",
    "\n",
    "        self.register_buffer(\"phi\", torch.zeros(n_gmm))\n",
    "        self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n",
    "        self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n",
    "\n",
    "    def relative_euclidean_distance(self, a, b):\n",
    "        return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        rec_cosine = F.cosine_similarity(x, dec, dim=1)\n",
    "        rec_euclidean = self.relative_euclidean_distance(x, dec)\n",
    "\n",
    "        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "\n",
    "        gamma = self.estimation(z)\n",
    "\n",
    "        return enc, dec, z, gamma\n",
    "\n",
    "    def compute_gmm_params(self, z, gamma):\n",
    "        N = gamma.size(0)\n",
    "        # K\n",
    "        sum_gamma = torch.sum(gamma, dim=0)\n",
    "\n",
    "        # K\n",
    "        phi = (sum_gamma / N)\n",
    "\n",
    "        self.phi = phi.data\n",
    "\n",
    " \n",
    "        # K x D\n",
    "        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "        self.mu = mu.data\n",
    "        # z = N x D\n",
    "        # mu = K x D\n",
    "        # gamma N x K\n",
    "\n",
    "        # z_mu = N x K x D\n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "\n",
    "        # z_mu_outer = N x K x D x D\n",
    "        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "\n",
    "        # K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        self.cov = cov.data\n",
    "\n",
    "        return phi, mu, cov\n",
    "        \n",
    "    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "        if phi is None:\n",
    "            phi = to_var(self.phi)\n",
    "        if mu is None:\n",
    "            mu = to_var(self.mu)\n",
    "        if cov is None:\n",
    "            cov = to_var(self.cov)\n",
    "\n",
    "        k, D, _ = cov.size()\n",
    "\n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = 1e-12\n",
    "        for i in range(k):\n",
    "            # K x D x D\n",
    "            cov_k = cov[i] + to_var(torch.eye(D)*eps)\n",
    "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "\n",
    "            #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n",
    "            det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n",
    "            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "\n",
    "        # K x D x D\n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        # K\n",
    "        det_cov = torch.cat(det_cov).cuda()\n",
    "        #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "\n",
    "        # N x K\n",
    "        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        # for stability (logsumexp)\n",
    "        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "\n",
    "        exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "\n",
    "        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n",
    "        sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n",
    "        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n",
    "\n",
    "\n",
    "        if size_average:\n",
    "            sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "\n",
    "    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "\n",
    "        recon_error = torch.mean((x - x_hat) ** 2)\n",
    "\n",
    "        phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "\n",
    "        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "\n",
    "        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "\n",
    "        return loss, sample_energy, recon_error, cov_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "586c4341-12b6-4c3b-ab9e-1b9ac8b29a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    DEFAULTS = {}   \n",
    "    def __init__(self, data_loader, config):\n",
    "        # Data loader\n",
    "        self.__dict__.update(Solver.DEFAULTS, **config)\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        # Build tensorboard if use\n",
    "        self.build_model()\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            self.load_pretrained_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Define model\n",
    "        self.dagmm = DaGMM(self.gmm_k)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n",
    "\n",
    "        # Print networks\n",
    "        self.print_network(self.dagmm, 'DaGMM')\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.dagmm.cuda()\n",
    "\n",
    "    def print_network(self, model, name):\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(name)\n",
    "        print(model)\n",
    "        print(\"The number of parameters: {}\".format(num_params))\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.dagmm.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n",
    "\n",
    "        print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n",
    "\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
    "\n",
    "    def build_tensorboard(self):\n",
    "        from logger import Logger\n",
    "        self.logger = Logger(self.log_path)\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.dagmm.zero_grad()\n",
    "\n",
    "    def to_var(self, x, volatile=False):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return Variable(x, volatile=volatile)\n",
    "\n",
    "    def train(self):\n",
    "        iters_per_epoch = len(self.data_loader)\n",
    "\n",
    "        # Start with trained model if exists\n",
    "        if self.pretrained_model:\n",
    "            start = int(self.pretrained_model.split('_')[0])\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        # Start training\n",
    "        iter_ctr = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        self.ap_global_train = np.array([0,0,0])\n",
    "        for e in range(start, self.num_epochs):\n",
    "            for i, (input_data, labels) in enumerate(tqdm(self.data_loader)):\n",
    "                iter_ctr += 1\n",
    "                start = time.time()\n",
    "\n",
    "                input_data = self.to_var(input_data)\n",
    "\n",
    "                total_loss,sample_energy, recon_error, cov_diag = self.dagmm_step(input_data)\n",
    "                # Logging\n",
    "                loss = {}\n",
    "                loss['total_loss'] = total_loss.data.item()\n",
    "                loss['sample_energy'] = sample_energy.item()\n",
    "                loss['recon_error'] = recon_error.item()\n",
    "                loss['cov_diag'] = cov_diag.item()\n",
    "\n",
    "\n",
    "\n",
    "                # Print out log info\n",
    "                if (i+1) % self.log_step == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    total_time = ((self.num_epochs*iters_per_epoch)-(e*iters_per_epoch+i)) * elapsed/(e*iters_per_epoch+i+1)\n",
    "                    epoch_time = (iters_per_epoch-i)* elapsed/(e*iters_per_epoch+i+1)\n",
    "                    \n",
    "                    epoch_time = str(datetime.timedelta(seconds=epoch_time))\n",
    "                    total_time = str(datetime.timedelta(seconds=total_time))\n",
    "                    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "\n",
    "                    lr_tmp = []\n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        lr_tmp.append(param_group['lr'])\n",
    "                    tmplr = np.squeeze(np.array(lr_tmp))\n",
    "\n",
    "                    log = \"Elapsed {}/{} -- {} , Epoch [{}/{}], Iter [{}/{}], lr {}\".format(\n",
    "                        elapsed,epoch_time,total_time, e+1, self.num_epochs, i+1, iters_per_epoch, tmplr)\n",
    "\n",
    "                    for tag, value in loss.items():\n",
    "                        log += \", {}: {:.4f}\".format(tag, value)\n",
    "\n",
    "                    IPython.display.clear_output()\n",
    "                    print(log)\n",
    "\n",
    "                    if self.use_tensorboard:\n",
    "                        for tag, value in loss.items():\n",
    "                            self.logger.scalar_summary(tag, value, e * iters_per_epoch + i + 1)\n",
    "                    else:\n",
    "                        plt_ctr = 1\n",
    "                        if not hasattr(self,\"loss_logs\"):\n",
    "                            self.loss_logs = {}\n",
    "                            for loss_key in loss:\n",
    "                                self.loss_logs[loss_key] = [loss[loss_key]]\n",
    "                                plt.subplot(2,2,plt_ctr)\n",
    "                                plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n",
    "                                plt.legend()\n",
    "                                plt_ctr += 1\n",
    "                        else:\n",
    "                            for loss_key in loss:\n",
    "                                self.loss_logs[loss_key].append(loss[loss_key])\n",
    "                                plt.subplot(2,2,plt_ctr)\n",
    "                                plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n",
    "                                plt.legend()\n",
    "                                plt_ctr += 1\n",
    "\n",
    "                        plt.show()\n",
    "\n",
    "                    print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n",
    "                # Save model checkpoints\n",
    "                if (i+1) % self.model_save_step == 0:\n",
    "                    torch.save(self.dagmm.state_dict(),\n",
    "                        os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1)))\n",
    "\n",
    "    def dagmm_step(self, input_data):\n",
    "        self.dagmm.train()\n",
    "        enc, dec, z, gamma = self.dagmm(input_data)\n",
    "\n",
    "        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lambda_energy, self.lambda_cov_diag)\n",
    "\n",
    "        self.reset_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss,sample_energy, recon_error, cov_diag\n",
    "\n",
    "    def test(self):\n",
    "        print(\"======================TEST MODE======================\")\n",
    "        self.dagmm.eval()\n",
    "        self.data_loader.dataset.mode=\"train\"\n",
    "\n",
    "        N = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        gamma_sum = 0\n",
    "\n",
    "        for it, (input_data, labels) in enumerate(self.data_loader):\n",
    "            input_data = self.to_var(input_data)\n",
    "            enc, dec, z, gamma = self.dagmm(input_data)\n",
    "            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n",
    "            \n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "            \n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n",
    "            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n",
    "            \n",
    "            N += input_data.size(0)\n",
    "            \n",
    "        train_phi = gamma_sum / N\n",
    "        train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "        train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        print(\"N:\",N)\n",
    "        print(\"phi :\\n\",train_phi)\n",
    "        print(\"mu :\\n\",train_mu)\n",
    "        print(\"cov :\\n\",train_cov)\n",
    "\n",
    "        train_energy = []\n",
    "        train_labels = []\n",
    "        train_z = []\n",
    "        for it, (input_data, labels) in enumerate(self.data_loader):\n",
    "            input_data = self.to_var(input_data)\n",
    "            enc, dec, z, gamma = self.dagmm(input_data)\n",
    "            sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n",
    "            \n",
    "            train_energy.append(sample_energy.data.cpu().numpy())\n",
    "            train_z.append(z.data.cpu().numpy())\n",
    "            train_labels.append(labels.numpy())\n",
    "\n",
    "\n",
    "        train_energy = np.concatenate(train_energy,axis=0)\n",
    "        train_z = np.concatenate(train_z,axis=0)\n",
    "        train_labels = np.concatenate(train_labels,axis=0)\n",
    "\n",
    "\n",
    "        self.data_loader.dataset.mode=\"test\"\n",
    "        test_energy = []\n",
    "        test_labels = []\n",
    "        test_z = []\n",
    "        for it, (input_data, labels) in enumerate(self.data_loader):\n",
    "            input_data = self.to_var(input_data)\n",
    "            enc, dec, z, gamma = self.dagmm(input_data)\n",
    "            sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n",
    "            test_energy.append(sample_energy.data.cpu().numpy())\n",
    "            test_z.append(z.data.cpu().numpy())\n",
    "            test_labels.append(labels.numpy())\n",
    "\n",
    "\n",
    "        test_energy = np.concatenate(test_energy,axis=0)\n",
    "        test_z = np.concatenate(test_z,axis=0)\n",
    "        test_labels = np.concatenate(test_labels,axis=0)\n",
    "\n",
    "        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n",
    "        combined_labels = np.concatenate([train_labels, test_labels], axis=0)\n",
    "\n",
    "        thresh = np.percentile(combined_energy, 100 - 20)\n",
    "        print(\"Threshold :\", thresh)\n",
    "\n",
    "        pred = (test_energy > thresh).astype(int)\n",
    "        gt = test_labels.astype(int)\n",
    "\n",
    "        from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n",
    "\n",
    "        accuracy = accuracy_score(gt,pred)\n",
    "        precision, recall, f_score, support = prf(gt, pred, average='binary')\n",
    "\n",
    "        print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n",
    "        \n",
    "        return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1290273-a73a-46ab-a3c5-211af7af6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=hyperparams(defaults)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
